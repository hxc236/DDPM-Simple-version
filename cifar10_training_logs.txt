Files already downloaded and verified
训练集大小: 40000
验证集大小: 10000
<torch.utils.data.dataloader.DataLoader object at 0x000001FEC5155220>
Training Epoch 0 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.5242787599563599
iteration 20 : mse loss = 0.31242722272872925
iteration 30 : mse loss = 0.2152779996395111
iteration 40 : mse loss = 0.15665802359580994
iteration 50 : mse loss = 0.1488819569349289
iteration 60 : mse loss = 0.1392797827720642
iteration 70 : mse loss = 0.13157130777835846
iteration 80 : mse loss = 0.12523028254508972
iteration 90 : mse loss = 0.11751984059810638
iteration 100 : mse loss = 0.11490790545940399
iteration 110 : mse loss = 0.13838909566402435
iteration 120 : mse loss = 0.11261295527219772
iteration 130 : mse loss = 0.10796773433685303
iteration 140 : mse loss = 0.08844726532697678
iteration 150 : mse loss = 0.09361045807600021
train mean loss: 0.18974827229976654
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.0936337485909462
Training Epoch 1 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.08317533880472183
iteration 20 : mse loss = 0.07903079688549042
iteration 30 : mse loss = 0.08370935171842575
iteration 40 : mse loss = 0.0797274112701416
iteration 50 : mse loss = 0.07850287109613419
iteration 60 : mse loss = 0.07883208990097046
iteration 70 : mse loss = 0.08459176868200302
iteration 80 : mse loss = 0.0742509514093399
iteration 90 : mse loss = 0.07275444269180298
iteration 100 : mse loss = 0.07091625034809113
iteration 110 : mse loss = 0.06316094100475311
iteration 120 : mse loss = 0.0687611848115921
iteration 130 : mse loss = 0.060271136462688446
iteration 140 : mse loss = 0.07291548699140549
iteration 150 : mse loss = 0.0669986754655838
train mean loss: 0.07585509866476059
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.06543822586536407
Training Epoch 2 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.0724296048283577
iteration 20 : mse loss = 0.06488907337188721
iteration 30 : mse loss = 0.06352968513965607
iteration 40 : mse loss = 0.061240583658218384
iteration 50 : mse loss = 0.08029340207576752
iteration 60 : mse loss = 0.06004717946052551
iteration 70 : mse loss = 0.06508446484804153
iteration 80 : mse loss = 0.05275779962539673
iteration 90 : mse loss = 0.05577053129673004
iteration 100 : mse loss = 0.060954898595809937
iteration 110 : mse loss = 0.05939941853284836
iteration 120 : mse loss = 0.057533539831638336
iteration 130 : mse loss = 0.0626377984881401
iteration 140 : mse loss = 0.05956534668803215
iteration 150 : mse loss = 0.0673917755484581
train mean loss: 0.0613802894949913
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.057452648878097534
Training Epoch 3 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.06138334795832634
iteration 20 : mse loss = 0.061237167567014694
iteration 30 : mse loss = 0.052039943635463715
iteration 40 : mse loss = 0.07289231568574905
iteration 50 : mse loss = 0.04709726199507713
iteration 60 : mse loss = 0.04436151310801506
iteration 70 : mse loss = 0.05892321467399597
iteration 80 : mse loss = 0.0510292612016201
iteration 90 : mse loss = 0.06048557162284851
iteration 100 : mse loss = 0.05357560142874718
iteration 110 : mse loss = 0.04850391671061516
iteration 120 : mse loss = 0.05001722276210785
iteration 130 : mse loss = 0.04879217594861984
iteration 140 : mse loss = 0.05294693633913994
iteration 150 : mse loss = 0.05523458123207092
train mean loss: 0.054733797907829285
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.05312517285346985
Training Epoch 4 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.04274221882224083
iteration 20 : mse loss = 0.05616183206439018
iteration 30 : mse loss = 0.0427822470664978
iteration 40 : mse loss = 0.050579991191625595
iteration 50 : mse loss = 0.05127957463264465
iteration 60 : mse loss = 0.05004680156707764
iteration 70 : mse loss = 0.05634621903300285
iteration 80 : mse loss = 0.0577390193939209
iteration 90 : mse loss = 0.06275966763496399
iteration 100 : mse loss = 0.04921471327543259
iteration 110 : mse loss = 0.04695058614015579
iteration 120 : mse loss = 0.0477827712893486
iteration 130 : mse loss = 0.04833628982305527
iteration 140 : mse loss = 0.047877416014671326
iteration 150 : mse loss = 0.04360344260931015
train mean loss: 0.050142787396907806
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.047120533883571625
Training Epoch 5 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.04975268989801407
iteration 20 : mse loss = 0.0459987074136734
iteration 30 : mse loss = 0.04991283267736435
iteration 40 : mse loss = 0.047038670629262924
iteration 50 : mse loss = 0.037881605327129364
iteration 60 : mse loss = 0.04134352505207062
iteration 70 : mse loss = 0.04474054276943207
iteration 80 : mse loss = 0.04715680330991745
iteration 90 : mse loss = 0.043307941406965256
iteration 100 : mse loss = 0.04957053065299988
iteration 110 : mse loss = 0.04248166084289551
iteration 120 : mse loss = 0.04219912737607956
iteration 130 : mse loss = 0.04096834361553192
iteration 140 : mse loss = 0.0477450005710125
iteration 150 : mse loss = 0.05213389918208122
train mean loss: 0.04683513939380646
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.04580823332071304
Training Epoch 6 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.04240338131785393
iteration 20 : mse loss = 0.05895319581031799
iteration 30 : mse loss = 0.053598593920469284
iteration 40 : mse loss = 0.04890182614326477
iteration 50 : mse loss = 0.04637911915779114
iteration 60 : mse loss = 0.04845196008682251
iteration 70 : mse loss = 0.04885881766676903
iteration 80 : mse loss = 0.05024969205260277
iteration 90 : mse loss = 0.05795052647590637
iteration 100 : mse loss = 0.04152897372841835
iteration 110 : mse loss = 0.04514991492033005
iteration 120 : mse loss = 0.04784473031759262
iteration 130 : mse loss = 0.03850550204515457
iteration 140 : mse loss = 0.052134715020656586
iteration 150 : mse loss = 0.04521164298057556
train mean loss: 0.04623560979962349
early_stop_time/early_stop_threshold: 1 / 40, valid mean loss: 0.04624107480049133
Training Epoch 7 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.036324985325336456
iteration 20 : mse loss = 0.03697552904486656
iteration 30 : mse loss = 0.04663104936480522
iteration 40 : mse loss = 0.04610732942819595
iteration 50 : mse loss = 0.04570680484175682
iteration 60 : mse loss = 0.039388757199048996
iteration 70 : mse loss = 0.0441133975982666
iteration 80 : mse loss = 0.04141679406166077
iteration 90 : mse loss = 0.0382053516805172
iteration 100 : mse loss = 0.05027907341718674
iteration 110 : mse loss = 0.03143778070807457
iteration 120 : mse loss = 0.048300206661224365
iteration 130 : mse loss = 0.038703665137290955
iteration 140 : mse loss = 0.053155578672885895
iteration 150 : mse loss = 0.0397341325879097
train mean loss: 0.043721042573451996
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.04269888252019882
Training Epoch 8 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.045267753303050995
iteration 20 : mse loss = 0.03674740344285965
iteration 30 : mse loss = 0.04601907730102539
iteration 40 : mse loss = 0.03412090241909027
iteration 50 : mse loss = 0.04199083149433136
iteration 60 : mse loss = 0.040379032492637634
iteration 70 : mse loss = 0.048331063240766525
iteration 80 : mse loss = 0.038758695125579834
iteration 90 : mse loss = 0.03778993338346481
iteration 100 : mse loss = 0.03966166824102402
iteration 110 : mse loss = 0.03759826719760895
iteration 120 : mse loss = 0.04515507444739342
iteration 130 : mse loss = 0.037491995841264725
iteration 140 : mse loss = 0.05142899602651596
iteration 150 : mse loss = 0.04104066640138626
train mean loss: 0.04390313848853111
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.042317673563957214
Training Epoch 9 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.037974778562784195
iteration 20 : mse loss = 0.044579774141311646
iteration 30 : mse loss = 0.03575599938631058
iteration 40 : mse loss = 0.03845562785863876
iteration 50 : mse loss = 0.03525524586439133
iteration 60 : mse loss = 0.04389381408691406
iteration 70 : mse loss = 0.04755806177854538
iteration 80 : mse loss = 0.036065444350242615
iteration 90 : mse loss = 0.03318563848733902
iteration 100 : mse loss = 0.03246995434165001
iteration 110 : mse loss = 0.04386743903160095
iteration 120 : mse loss = 0.042838916182518005
iteration 130 : mse loss = 0.04114533215761185
iteration 140 : mse loss = 0.039655014872550964
iteration 150 : mse loss = 0.05138406902551651
train mean loss: 0.04178616777062416
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.04165700450539589
Training Epoch 10 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.04082223400473595
iteration 20 : mse loss = 0.04210968315601349
iteration 30 : mse loss = 0.04558825492858887
iteration 40 : mse loss = 0.045400362461805344
iteration 50 : mse loss = 0.05096499249339104
iteration 60 : mse loss = 0.032814182341098785
iteration 70 : mse loss = 0.0404612198472023
iteration 80 : mse loss = 0.03470662236213684
iteration 90 : mse loss = 0.03840825706720352
iteration 100 : mse loss = 0.042339175939559937
iteration 110 : mse loss = 0.034590356051921844
iteration 120 : mse loss = 0.04414428398013115
iteration 130 : mse loss = 0.0344640389084816
iteration 140 : mse loss = 0.04816659539937973
iteration 150 : mse loss = 0.04257906600832939
train mean loss: 0.040246348828077316
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.04081360623240471
Training Epoch 11 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.04103200137615204
iteration 20 : mse loss = 0.04527820274233818
iteration 30 : mse loss = 0.05190160498023033
iteration 40 : mse loss = 0.03896059840917587
iteration 50 : mse loss = 0.04365157708525658
iteration 60 : mse loss = 0.046730123460292816
iteration 70 : mse loss = 0.038396503776311874
iteration 80 : mse loss = 0.03918607532978058
iteration 90 : mse loss = 0.040839407593011856
iteration 100 : mse loss = 0.04596472531557083
iteration 110 : mse loss = 0.05036269128322601
iteration 120 : mse loss = 0.03695685416460037
iteration 130 : mse loss = 0.04376775771379471
iteration 140 : mse loss = 0.04421418905258179
iteration 150 : mse loss = 0.03636763617396355
train mean loss: 0.040156833827495575
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.039303530007600784
Training Epoch 12 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.0332326665520668
iteration 20 : mse loss = 0.04162672907114029
iteration 30 : mse loss = 0.034195639193058014
iteration 40 : mse loss = 0.04055642709136009
iteration 50 : mse loss = 0.05440653860569
iteration 60 : mse loss = 0.03407898545265198
iteration 70 : mse loss = 0.03320782631635666
iteration 80 : mse loss = 0.032999392598867416
iteration 90 : mse loss = 0.041909970343112946
iteration 100 : mse loss = 0.03494206815958023
iteration 110 : mse loss = 0.04087911173701286
iteration 120 : mse loss = 0.04485083743929863
iteration 130 : mse loss = 0.039932020008563995
iteration 140 : mse loss = 0.03542661666870117
iteration 150 : mse loss = 0.03311467170715332
train mean loss: 0.039488907903432846
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.03850429505109787
Training Epoch 13 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03654418885707855
iteration 20 : mse loss = 0.038557857275009155
iteration 30 : mse loss = 0.03671314939856529
iteration 40 : mse loss = 0.04112302511930466
iteration 50 : mse loss = 0.0406772717833519
iteration 60 : mse loss = 0.04233040660619736
iteration 70 : mse loss = 0.03269670531153679
iteration 80 : mse loss = 0.03629603236913681
iteration 90 : mse loss = 0.040118783712387085
iteration 100 : mse loss = 0.0370294563472271
iteration 110 : mse loss = 0.03969346731901169
iteration 120 : mse loss = 0.038455069065093994
iteration 130 : mse loss = 0.04252006858587265
iteration 140 : mse loss = 0.03261878713965416
iteration 150 : mse loss = 0.0342949740588665
train mean loss: 0.03892385587096214
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.0384318083524704
Training Epoch 14 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03902922943234444
iteration 20 : mse loss = 0.04338601231575012
iteration 30 : mse loss = 0.035845838487148285
iteration 40 : mse loss = 0.04633672162890434
iteration 50 : mse loss = 0.032170288264751434
iteration 60 : mse loss = 0.04228302091360092
iteration 70 : mse loss = 0.03448962792754173
iteration 80 : mse loss = 0.0448974184691906
iteration 90 : mse loss = 0.04121425747871399
iteration 100 : mse loss = 0.031144771724939346
iteration 110 : mse loss = 0.03813699632883072
iteration 120 : mse loss = 0.03825639188289642
iteration 130 : mse loss = 0.04479292780160904
iteration 140 : mse loss = 0.032463639974594116
iteration 150 : mse loss = 0.030492430552840233
train mean loss: 0.03856407478451729
early_stop_time/early_stop_threshold: 1 / 40, valid mean loss: 0.040004149079322815
Training Epoch 15 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.035550251603126526
iteration 20 : mse loss = 0.03607060760259628
iteration 30 : mse loss = 0.038727324455976486
iteration 40 : mse loss = 0.037185490131378174
iteration 50 : mse loss = 0.037710145115852356
iteration 60 : mse loss = 0.0361279658973217
iteration 70 : mse loss = 0.029383933171629906
iteration 80 : mse loss = 0.037820763885974884
iteration 90 : mse loss = 0.035513315349817276
iteration 100 : mse loss = 0.03078283928334713
iteration 110 : mse loss = 0.03220919519662857
iteration 120 : mse loss = 0.04383137822151184
iteration 130 : mse loss = 0.034844622015953064
iteration 140 : mse loss = 0.04102092236280441
iteration 150 : mse loss = 0.04517623782157898
train mean loss: 0.037368226796388626
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.03781265392899513
Training Epoch 16 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03473478555679321
iteration 20 : mse loss = 0.033980026841163635
iteration 30 : mse loss = 0.03132535517215729
iteration 40 : mse loss = 0.03877907991409302
iteration 50 : mse loss = 0.04650580883026123
iteration 60 : mse loss = 0.04048832878470421
iteration 70 : mse loss = 0.04380720853805542
iteration 80 : mse loss = 0.038041889667510986
iteration 90 : mse loss = 0.041969671845436096
iteration 100 : mse loss = 0.04100380837917328
iteration 110 : mse loss = 0.03917340934276581
iteration 120 : mse loss = 0.03166775405406952
iteration 130 : mse loss = 0.03451448678970337
iteration 140 : mse loss = 0.04221952706575394
iteration 150 : mse loss = 0.029152972623705864
train mean loss: 0.03774596378207207
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.037301477044820786
Training Epoch 17 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.04027942568063736
iteration 20 : mse loss = 0.03585699200630188
iteration 30 : mse loss = 0.04196074604988098
iteration 40 : mse loss = 0.040154460817575455
iteration 50 : mse loss = 0.04090224206447601
iteration 60 : mse loss = 0.037656232714653015
iteration 70 : mse loss = 0.03465937450528145
iteration 80 : mse loss = 0.029339348897337914
iteration 90 : mse loss = 0.0313897505402565
iteration 100 : mse loss = 0.03567859157919884
iteration 110 : mse loss = 0.029483487829566002
iteration 120 : mse loss = 0.04285970330238342
iteration 130 : mse loss = 0.02925751358270645
iteration 140 : mse loss = 0.0320364311337471
iteration 150 : mse loss = 0.036120153963565826
train mean loss: 0.03736678510904312
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.036030642688274384
Training Epoch 18 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03319733217358589
iteration 20 : mse loss = 0.03501354902982712
iteration 30 : mse loss = 0.03989206999540329
iteration 40 : mse loss = 0.03563205152750015
iteration 50 : mse loss = 0.04685298353433609
iteration 60 : mse loss = 0.04381705820560455
iteration 70 : mse loss = 0.033660512417554855
iteration 80 : mse loss = 0.03339291363954544
iteration 90 : mse loss = 0.03552984446287155
iteration 100 : mse loss = 0.03776552528142929
iteration 110 : mse loss = 0.03651687130331993
iteration 120 : mse loss = 0.029100459069013596
iteration 130 : mse loss = 0.0467451810836792
iteration 140 : mse loss = 0.037757210433483124
iteration 150 : mse loss = 0.04300951585173607
train mean loss: 0.037151068449020386
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.03566044196486473
Training Epoch 19 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03602750226855278
iteration 20 : mse loss = 0.03707248717546463
iteration 30 : mse loss = 0.04016995429992676
iteration 40 : mse loss = 0.03728892654180527
iteration 50 : mse loss = 0.031707242131233215
iteration 60 : mse loss = 0.03783586248755455
iteration 70 : mse loss = 0.0376533642411232
iteration 80 : mse loss = 0.039383068680763245
iteration 90 : mse loss = 0.039011966437101364
iteration 100 : mse loss = 0.03723057731986046
iteration 110 : mse loss = 0.0392022542655468
iteration 120 : mse loss = 0.028809454292058945
iteration 130 : mse loss = 0.03731655701994896
iteration 140 : mse loss = 0.04068145900964737
iteration 150 : mse loss = 0.027245236560702324
train mean loss: 0.036926690489053726
early_stop_time/early_stop_threshold: 1 / 40, valid mean loss: 0.03693529963493347
Training Epoch 20 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.04218710586428642
iteration 20 : mse loss = 0.031602777540683746
iteration 30 : mse loss = 0.03960356116294861
iteration 40 : mse loss = 0.038975879549980164
iteration 50 : mse loss = 0.04070565849542618
iteration 60 : mse loss = 0.025729529559612274
iteration 70 : mse loss = 0.031586967408657074
iteration 80 : mse loss = 0.03139209374785423
iteration 90 : mse loss = 0.04483339190483093
iteration 100 : mse loss = 0.03642546758055687
iteration 110 : mse loss = 0.03651121258735657
iteration 120 : mse loss = 0.040747664868831635
iteration 130 : mse loss = 0.0386265330016613
iteration 140 : mse loss = 0.04052859544754028
iteration 150 : mse loss = 0.040457919239997864
train mean loss: 0.03666505217552185
early_stop_time/early_stop_threshold: 2 / 40, valid mean loss: 0.03759314492344856
Training Epoch 21 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.0324799120426178
iteration 20 : mse loss = 0.03831616789102554
iteration 30 : mse loss = 0.03091026097536087
iteration 40 : mse loss = 0.035104598850011826
iteration 50 : mse loss = 0.03689339756965637
iteration 60 : mse loss = 0.033396199345588684
iteration 70 : mse loss = 0.034374311566352844
iteration 80 : mse loss = 0.03079981729388237
iteration 90 : mse loss = 0.037950579077005386
iteration 100 : mse loss = 0.03423222154378891
iteration 110 : mse loss = 0.03147595375776291
iteration 120 : mse loss = 0.030787035822868347
iteration 130 : mse loss = 0.03908506780862808
iteration 140 : mse loss = 0.0433499738574028
iteration 150 : mse loss = 0.03604361414909363
train mean loss: 0.03577214106917381
early_stop_time/early_stop_threshold: 3 / 40, valid mean loss: 0.03776823729276657
Training Epoch 22 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.040819261223077774
iteration 20 : mse loss = 0.034679021686315536
iteration 30 : mse loss = 0.034273725003004074
iteration 40 : mse loss = 0.029770687222480774
iteration 50 : mse loss = 0.02834637090563774
iteration 60 : mse loss = 0.0296337753534317
iteration 70 : mse loss = 0.033594608306884766
iteration 80 : mse loss = 0.03565597161650658
iteration 90 : mse loss = 0.030687686055898666
iteration 100 : mse loss = 0.029978975653648376
iteration 110 : mse loss = 0.03385030850768089
iteration 120 : mse loss = 0.046446990221738815
iteration 130 : mse loss = 0.03648775815963745
iteration 140 : mse loss = 0.03935405611991882
iteration 150 : mse loss = 0.03618856146931648
train mean loss: 0.035948727279901505
early_stop_time/early_stop_threshold: 4 / 40, valid mean loss: 0.03601018711924553
Training Epoch 23 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.04074074327945709
iteration 20 : mse loss = 0.03424164280295372
iteration 30 : mse loss = 0.030826523900032043
iteration 40 : mse loss = 0.03355153650045395
iteration 50 : mse loss = 0.030715202912688255
iteration 60 : mse loss = 0.031897515058517456
iteration 70 : mse loss = 0.0335940346121788
iteration 80 : mse loss = 0.034256186336278915
iteration 90 : mse loss = 0.0478191003203392
iteration 100 : mse loss = 0.03860321640968323
iteration 110 : mse loss = 0.031756721436977386
iteration 120 : mse loss = 0.03167136013507843
iteration 130 : mse loss = 0.03910540044307709
iteration 140 : mse loss = 0.04028785973787308
iteration 150 : mse loss = 0.033965304493904114
train mean loss: 0.03581571578979492
early_stop_time/early_stop_threshold: 5 / 40, valid mean loss: 0.03666406124830246
Training Epoch 24 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03477597236633301
iteration 20 : mse loss = 0.035516202449798584
iteration 30 : mse loss = 0.03800254687666893
iteration 40 : mse loss = 0.03336086496710777
iteration 50 : mse loss = 0.03559347987174988
iteration 60 : mse loss = 0.04338997229933739
iteration 70 : mse loss = 0.036095596849918365
iteration 80 : mse loss = 0.03729718551039696
iteration 90 : mse loss = 0.03693917393684387
iteration 100 : mse loss = 0.0327584482729435
iteration 110 : mse loss = 0.033813029527664185
iteration 120 : mse loss = 0.03636813536286354
iteration 130 : mse loss = 0.04614069312810898
iteration 140 : mse loss = 0.02978660725057125
iteration 150 : mse loss = 0.03705834224820137
train mean loss: 0.0357620008289814
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.03497656062245369
Training Epoch 25 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.0329502671957016
iteration 20 : mse loss = 0.03466857969760895
iteration 30 : mse loss = 0.0380084291100502
iteration 40 : mse loss = 0.040243834257125854
iteration 50 : mse loss = 0.0335393063724041
iteration 60 : mse loss = 0.037522461265325546
iteration 70 : mse loss = 0.03462629392743111
iteration 80 : mse loss = 0.03643137216567993
iteration 90 : mse loss = 0.04071613401174545
iteration 100 : mse loss = 0.03544425964355469
iteration 110 : mse loss = 0.03754065930843353
iteration 120 : mse loss = 0.029391176998615265
iteration 130 : mse loss = 0.03885501250624657
iteration 140 : mse loss = 0.03606121242046356
iteration 150 : mse loss = 0.038320060819387436
train mean loss: 0.03507767617702484
early_stop_time/early_stop_threshold: 1 / 40, valid mean loss: 0.03731067478656769
Training Epoch 26 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03411393612623215
iteration 20 : mse loss = 0.030626974999904633
iteration 30 : mse loss = 0.03650062903761864
iteration 40 : mse loss = 0.03258895128965378
iteration 50 : mse loss = 0.04154069721698761
iteration 60 : mse loss = 0.03455152362585068
iteration 70 : mse loss = 0.029588498175144196
iteration 80 : mse loss = 0.035233449190855026
iteration 90 : mse loss = 0.037729158997535706
iteration 100 : mse loss = 0.04272513836622238
iteration 110 : mse loss = 0.03676360845565796
iteration 120 : mse loss = 0.03164447098970413
iteration 130 : mse loss = 0.033882249146699905
iteration 140 : mse loss = 0.03428877517580986
iteration 150 : mse loss = 0.046393297612667084
train mean loss: 0.03525731712579727
early_stop_time/early_stop_threshold: 2 / 40, valid mean loss: 0.03518468141555786
Training Epoch 27 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.029711376875638962
iteration 20 : mse loss = 0.029990732669830322
iteration 30 : mse loss = 0.032335784286260605
iteration 40 : mse loss = 0.03767367824912071
iteration 50 : mse loss = 0.03042498603463173
iteration 60 : mse loss = 0.03423271328210831
iteration 70 : mse loss = 0.036250218749046326
iteration 80 : mse loss = 0.03392450138926506
iteration 90 : mse loss = 0.04149337857961655
iteration 100 : mse loss = 0.029513506218791008
iteration 110 : mse loss = 0.03329603374004364
iteration 120 : mse loss = 0.026739289984107018
iteration 130 : mse loss = 0.04378009960055351
iteration 140 : mse loss = 0.028222110122442245
iteration 150 : mse loss = 0.03482670336961746
train mean loss: 0.03554866090416908
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.03418120741844177
Training Epoch 28 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.033841051161289215
iteration 20 : mse loss = 0.037805430591106415
iteration 30 : mse loss = 0.027681808918714523
iteration 40 : mse loss = 0.03479333594441414
iteration 50 : mse loss = 0.0405627079308033
iteration 60 : mse loss = 0.03351175785064697
iteration 70 : mse loss = 0.033163830637931824
iteration 80 : mse loss = 0.03810400888323784
iteration 90 : mse loss = 0.03048061951994896
iteration 100 : mse loss = 0.03485502302646637
iteration 110 : mse loss = 0.032911267131567
iteration 120 : mse loss = 0.02811461314558983
iteration 130 : mse loss = 0.037566281855106354
iteration 140 : mse loss = 0.031839482486248016
iteration 150 : mse loss = 0.04443595930933952
train mean loss: 0.034444186836481094
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.03286144882440567
Training Epoch 29 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.029544726014137268
iteration 20 : mse loss = 0.027760863304138184
iteration 30 : mse loss = 0.03410780802369118
iteration 40 : mse loss = 0.042625319212675095
iteration 50 : mse loss = 0.03559398278594017
iteration 60 : mse loss = 0.03984063118696213
iteration 70 : mse loss = 0.044284507632255554
iteration 80 : mse loss = 0.030288513749837875
iteration 90 : mse loss = 0.03580779582262039
iteration 100 : mse loss = 0.027645200490951538
iteration 110 : mse loss = 0.04140187054872513
iteration 120 : mse loss = 0.034498583525419235
iteration 130 : mse loss = 0.03078010305762291
iteration 140 : mse loss = 0.031014200299978256
iteration 150 : mse loss = 0.043261438608169556
train mean loss: 0.035325732082128525
early_stop_time/early_stop_threshold: 1 / 40, valid mean loss: 0.03432341665029526
Training Epoch 30 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.036103300750255585
iteration 20 : mse loss = 0.03983026742935181
iteration 30 : mse loss = 0.03498901054263115
iteration 40 : mse loss = 0.031678706407547
iteration 50 : mse loss = 0.036665093153715134
iteration 60 : mse loss = 0.03265772759914398
iteration 70 : mse loss = 0.03169340640306473
iteration 80 : mse loss = 0.030896661803126335
iteration 90 : mse loss = 0.03572947159409523
iteration 100 : mse loss = 0.03174947202205658
iteration 110 : mse loss = 0.034930527210235596
iteration 120 : mse loss = 0.033101316541433334
iteration 130 : mse loss = 0.03139425441622734
iteration 140 : mse loss = 0.03478127345442772
iteration 150 : mse loss = 0.03851419687271118
train mean loss: 0.034779224544763565
early_stop_time/early_stop_threshold: 2 / 40, valid mean loss: 0.03529889136552811
Training Epoch 31 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.036984704434871674
iteration 20 : mse loss = 0.03227413818240166
iteration 30 : mse loss = 0.03258223459124565
iteration 40 : mse loss = 0.04111950844526291
iteration 50 : mse loss = 0.02695009671151638
iteration 60 : mse loss = 0.033599771559238434
iteration 70 : mse loss = 0.03174480050802231
iteration 80 : mse loss = 0.033263519406318665
iteration 90 : mse loss = 0.037379924207925797
iteration 100 : mse loss = 0.027439497411251068
iteration 110 : mse loss = 0.02962878905236721
iteration 120 : mse loss = 0.03425769507884979
iteration 130 : mse loss = 0.03211929649114609
iteration 140 : mse loss = 0.04621835798025131
iteration 150 : mse loss = 0.030212752521038055
train mean loss: 0.03469816595315933
early_stop_time/early_stop_threshold: 3 / 40, valid mean loss: 0.035522088408470154
Training Epoch 32 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.041684072464704514
iteration 20 : mse loss = 0.02891148254275322
iteration 30 : mse loss = 0.038578569889068604
iteration 40 : mse loss = 0.0267578586935997
iteration 50 : mse loss = 0.03551078960299492
iteration 60 : mse loss = 0.03529304265975952
iteration 70 : mse loss = 0.028680678457021713
iteration 80 : mse loss = 0.029520541429519653
iteration 90 : mse loss = 0.03436528146266937
iteration 100 : mse loss = 0.041565585881471634
iteration 110 : mse loss = 0.04023335501551628
iteration 120 : mse loss = 0.0364028625190258
iteration 130 : mse loss = 0.03664073348045349
iteration 140 : mse loss = 0.029373038560152054
iteration 150 : mse loss = 0.030258305370807648
train mean loss: 0.03505059704184532
early_stop_time/early_stop_threshold: 4 / 40, valid mean loss: 0.03654571622610092
Training Epoch 33 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.030510514974594116
iteration 20 : mse loss = 0.03446154668927193
iteration 30 : mse loss = 0.03431271016597748
iteration 40 : mse loss = 0.0348258838057518
iteration 50 : mse loss = 0.030029699206352234
iteration 60 : mse loss = 0.03112703561782837
iteration 70 : mse loss = 0.03402934968471527
iteration 80 : mse loss = 0.023875683546066284
iteration 90 : mse loss = 0.03411070629954338
iteration 100 : mse loss = 0.0376102589070797
iteration 110 : mse loss = 0.036520205438137054
iteration 120 : mse loss = 0.035551633685827255
iteration 130 : mse loss = 0.031716249883174896
iteration 140 : mse loss = 0.03922637924551964
iteration 150 : mse loss = 0.026692107319831848
train mean loss: 0.03422490879893303
early_stop_time/early_stop_threshold: 5 / 40, valid mean loss: 0.03406143933534622
Training Epoch 34 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.029084380716085434
iteration 20 : mse loss = 0.04118577763438225
iteration 30 : mse loss = 0.030541446059942245
iteration 40 : mse loss = 0.03697563707828522
iteration 50 : mse loss = 0.0345638170838356
iteration 60 : mse loss = 0.029004227370023727
iteration 70 : mse loss = 0.03394211828708649
iteration 80 : mse loss = 0.030955582857131958
iteration 90 : mse loss = 0.03560715168714523
iteration 100 : mse loss = 0.03057439997792244
iteration 110 : mse loss = 0.024929072707891464
iteration 120 : mse loss = 0.030120056122541428
iteration 130 : mse loss = 0.03089958429336548
iteration 140 : mse loss = 0.03476981818675995
iteration 150 : mse loss = 0.03347790986299515
train mean loss: 0.03370676189661026
early_stop_time/early_stop_threshold: 6 / 40, valid mean loss: 0.034806832671165466
Training Epoch 35 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.035331111401319504
iteration 20 : mse loss = 0.028387626633048058
iteration 30 : mse loss = 0.02901693992316723
iteration 40 : mse loss = 0.035553012043237686
iteration 50 : mse loss = 0.03462304174900055
iteration 60 : mse loss = 0.037099555134773254
iteration 70 : mse loss = 0.04388441890478134
iteration 80 : mse loss = 0.034962352365255356
iteration 90 : mse loss = 0.03096189722418785
iteration 100 : mse loss = 0.031408458948135376
iteration 110 : mse loss = 0.028235558420419693
iteration 120 : mse loss = 0.02802623063325882
iteration 130 : mse loss = 0.039890043437480927
iteration 140 : mse loss = 0.028608066961169243
iteration 150 : mse loss = 0.03606431186199188
train mean loss: 0.03445948287844658
early_stop_time/early_stop_threshold: 7 / 40, valid mean loss: 0.03286605700850487
Training Epoch 36 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03656082600355148
iteration 20 : mse loss = 0.04128916561603546
iteration 30 : mse loss = 0.03534021973609924
iteration 40 : mse loss = 0.031026965007185936
iteration 50 : mse loss = 0.034050844609737396
iteration 60 : mse loss = 0.028677767142653465
iteration 70 : mse loss = 0.028489384800195694
iteration 80 : mse loss = 0.04307270050048828
iteration 90 : mse loss = 0.03852104768157005
iteration 100 : mse loss = 0.03273855522274971
iteration 110 : mse loss = 0.029346562922000885
iteration 120 : mse loss = 0.03127845376729965
iteration 130 : mse loss = 0.027303194627165794
iteration 140 : mse loss = 0.030293762683868408
iteration 150 : mse loss = 0.027815308421850204
train mean loss: 0.034387048333883286
early_stop_time/early_stop_threshold: 8 / 40, valid mean loss: 0.03457493707537651
Training Epoch 37 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.031164666637778282
iteration 20 : mse loss = 0.037119876593351364
iteration 30 : mse loss = 0.032308682799339294
iteration 40 : mse loss = 0.028876906260848045
iteration 50 : mse loss = 0.0272396020591259
iteration 60 : mse loss = 0.03963979333639145
iteration 70 : mse loss = 0.03908603638410568
iteration 80 : mse loss = 0.03902825340628624
iteration 90 : mse loss = 0.037441518157720566
iteration 100 : mse loss = 0.0329279899597168
iteration 110 : mse loss = 0.03299454599618912
iteration 120 : mse loss = 0.03192561864852905
iteration 130 : mse loss = 0.03142473101615906
iteration 140 : mse loss = 0.04235095903277397
iteration 150 : mse loss = 0.034745365381240845
train mean loss: 0.03474069759249687
early_stop_time/early_stop_threshold: 9 / 40, valid mean loss: 0.034857653081417084
Training Epoch 38 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.029130054637789726
iteration 20 : mse loss = 0.030295534059405327
iteration 30 : mse loss = 0.033459462225437164
iteration 40 : mse loss = 0.04028811678290367
iteration 50 : mse loss = 0.037823840975761414
iteration 60 : mse loss = 0.029370084404945374
iteration 70 : mse loss = 0.03077705018222332
iteration 80 : mse loss = 0.03950373828411102
iteration 90 : mse loss = 0.04085179790854454
iteration 100 : mse loss = 0.02990449033677578
iteration 110 : mse loss = 0.03165459632873535
iteration 120 : mse loss = 0.04424121230840683
iteration 130 : mse loss = 0.037968710064888
iteration 140 : mse loss = 0.02563852071762085
iteration 150 : mse loss = 0.03791078180074692
train mean loss: 0.0342230387032032
early_stop_time/early_stop_threshold: 10 / 40, valid mean loss: 0.03479984402656555
Training Epoch 39 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.029723845422267914
iteration 20 : mse loss = 0.03606075048446655
iteration 30 : mse loss = 0.026657234877347946
iteration 40 : mse loss = 0.04184434562921524
iteration 50 : mse loss = 0.036014169454574585
iteration 60 : mse loss = 0.036793410778045654
iteration 70 : mse loss = 0.04276605695486069
iteration 80 : mse loss = 0.16223521530628204
iteration 90 : mse loss = 0.0558222234249115
iteration 100 : mse loss = 0.06565695255994797
iteration 110 : mse loss = 0.09608502686023712
iteration 120 : mse loss = 0.04777228459715843
iteration 130 : mse loss = 0.044031303375959396
iteration 140 : mse loss = 0.04425760731101036
iteration 150 : mse loss = 0.04957151412963867
train mean loss: 0.05058813840150833
early_stop_time/early_stop_threshold: 11 / 40, valid mean loss: 0.04618064686655998
Training Epoch 40 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.046910837292671204
iteration 20 : mse loss = 0.046048518270254135
iteration 30 : mse loss = 0.03622432053089142
iteration 40 : mse loss = 0.044091276824474335
iteration 50 : mse loss = 0.04112150892615318
iteration 60 : mse loss = 0.05092744156718254
iteration 70 : mse loss = 0.04413645714521408
iteration 80 : mse loss = 0.04205745458602905
iteration 90 : mse loss = 0.049666594713926315
iteration 100 : mse loss = 0.03712792694568634
iteration 110 : mse loss = 0.04929732903838158
iteration 120 : mse loss = 0.03835560381412506
iteration 130 : mse loss = 0.040004368871450424
iteration 140 : mse loss = 0.03935936465859413
iteration 150 : mse loss = 0.038690850138664246
train mean loss: 0.04411066696047783
early_stop_time/early_stop_threshold: 12 / 40, valid mean loss: 0.04036543518304825
Training Epoch 41 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.04131736606359482
iteration 20 : mse loss = 0.03941921889781952
iteration 30 : mse loss = 0.04631835222244263
iteration 40 : mse loss = 0.03628654032945633
iteration 50 : mse loss = 0.03030199371278286
iteration 60 : mse loss = 0.0414544977247715
iteration 70 : mse loss = 0.031887322664260864
iteration 80 : mse loss = 0.046134285628795624
iteration 90 : mse loss = 0.043846797198057175
iteration 100 : mse loss = 0.03433375805616379
iteration 110 : mse loss = 0.03843015804886818
iteration 120 : mse loss = 0.04255489632487297
iteration 130 : mse loss = 0.03220951557159424
iteration 140 : mse loss = 0.04835302755236626
iteration 150 : mse loss = 0.040469180792570114
train mean loss: 0.04006470739841461
early_stop_time/early_stop_threshold: 13 / 40, valid mean loss: 0.03903166577219963
Training Epoch 42 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03551095724105835
iteration 20 : mse loss = 0.04242224618792534
iteration 30 : mse loss = 0.03220708668231964
iteration 40 : mse loss = 0.040325626730918884
iteration 50 : mse loss = 0.04370405524969101
iteration 60 : mse loss = 0.04329059645533562
iteration 70 : mse loss = 0.033687956631183624
iteration 80 : mse loss = 0.04066799208521843
iteration 90 : mse loss = 0.0368618480861187
iteration 100 : mse loss = 0.04667234420776367
iteration 110 : mse loss = 0.0314570888876915
iteration 120 : mse loss = 0.04062715917825699
iteration 130 : mse loss = 0.03688208386301994
iteration 140 : mse loss = 0.03086111694574356
iteration 150 : mse loss = 0.047613244503736496
train mean loss: 0.038427308201789856
early_stop_time/early_stop_threshold: 14 / 40, valid mean loss: 0.03722912073135376
Training Epoch 43 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.04215977340936661
iteration 20 : mse loss = 0.03448764979839325
iteration 30 : mse loss = 0.0352771133184433
iteration 40 : mse loss = 0.03412047028541565
iteration 50 : mse loss = 0.03261241316795349
iteration 60 : mse loss = 0.03148413822054863
iteration 70 : mse loss = 0.03502444922924042
iteration 80 : mse loss = 0.04164327308535576
iteration 90 : mse loss = 0.03912410885095596
iteration 100 : mse loss = 0.03543240949511528
iteration 110 : mse loss = 0.03551603481173515
iteration 120 : mse loss = 0.04044567048549652
iteration 130 : mse loss = 0.0446167066693306
iteration 140 : mse loss = 0.03125520795583725
iteration 150 : mse loss = 0.04399290308356285
train mean loss: 0.03795376420021057
early_stop_time/early_stop_threshold: 15 / 40, valid mean loss: 0.03563059866428375
Training Epoch 44 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.042871907353401184
iteration 20 : mse loss = 0.03896540775895119
iteration 30 : mse loss = 0.04384493827819824
iteration 40 : mse loss = 0.04428539797663689
iteration 50 : mse loss = 0.037629447877407074
iteration 60 : mse loss = 0.03529837727546692
iteration 70 : mse loss = 0.03700006753206253
iteration 80 : mse loss = 0.03971005976200104
iteration 90 : mse loss = 0.03815736621618271
iteration 100 : mse loss = 0.03879937157034874
iteration 110 : mse loss = 0.042459115386009216
iteration 120 : mse loss = 0.039834458380937576
iteration 130 : mse loss = 0.03708098828792572
iteration 140 : mse loss = 0.03965166211128235
iteration 150 : mse loss = 0.04250985383987427
train mean loss: 0.037483371794223785
early_stop_time/early_stop_threshold: 16 / 40, valid mean loss: 0.036734215915203094
Training Epoch 45 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.028020327910780907
iteration 20 : mse loss = 0.02584834396839142
iteration 30 : mse loss = 0.04201787710189819
iteration 40 : mse loss = 0.037040770053863525
iteration 50 : mse loss = 0.041105955839157104
iteration 60 : mse loss = 0.03622232377529144
iteration 70 : mse loss = 0.04199926555156708
iteration 80 : mse loss = 0.034155480563640594
iteration 90 : mse loss = 0.04042758047580719
iteration 100 : mse loss = 0.02895473688840866
iteration 110 : mse loss = 0.03271883726119995
iteration 120 : mse loss = 0.03957970812916756
iteration 130 : mse loss = 0.026256844401359558
iteration 140 : mse loss = 0.03983356058597565
iteration 150 : mse loss = 0.04219294711947441
train mean loss: 0.03623106703162193
early_stop_time/early_stop_threshold: 17 / 40, valid mean loss: 0.03629923239350319
Training Epoch 46 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03715905547142029
iteration 20 : mse loss = 0.03218214958906174
iteration 30 : mse loss = 0.0433482751250267
iteration 40 : mse loss = 0.03259216994047165
iteration 50 : mse loss = 0.039793726056814194
iteration 60 : mse loss = 0.03388482332229614
iteration 70 : mse loss = 0.03606145456433296
iteration 80 : mse loss = 0.03681596368551254
iteration 90 : mse loss = 0.03216784819960594
iteration 100 : mse loss = 0.031490154564380646
iteration 110 : mse loss = 0.034496553242206573
iteration 120 : mse loss = 0.04359946772456169
iteration 130 : mse loss = 0.026337817311286926
iteration 140 : mse loss = 0.042981646955013275
iteration 150 : mse loss = 0.03464781865477562
train mean loss: 0.03646678477525711
early_stop_time/early_stop_threshold: 18 / 40, valid mean loss: 0.03571467846632004
Training Epoch 47 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03636154532432556
iteration 20 : mse loss = 0.03739491105079651
iteration 30 : mse loss = 0.041519369930028915
iteration 40 : mse loss = 0.04117659479379654
iteration 50 : mse loss = 0.042515091598033905
iteration 60 : mse loss = 0.033903978765010834
iteration 70 : mse loss = 0.0334603413939476
iteration 80 : mse loss = 0.04154806584119797
iteration 90 : mse loss = 0.033349838107824326
iteration 100 : mse loss = 0.034269943833351135
iteration 110 : mse loss = 0.043683409690856934
iteration 120 : mse loss = 0.028805606067180634
iteration 130 : mse loss = 0.029614582657814026
iteration 140 : mse loss = 0.04717717692255974
iteration 150 : mse loss = 0.031187355518341064
train mean loss: 0.03530129790306091
early_stop_time/early_stop_threshold: 19 / 40, valid mean loss: 0.03523752838373184
Training Epoch 48 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03304089605808258
iteration 20 : mse loss = 0.03735153377056122
iteration 30 : mse loss = 0.038160186260938644
iteration 40 : mse loss = 0.03319476172327995
iteration 50 : mse loss = 0.028630845248699188
iteration 60 : mse loss = 0.03965058922767639
iteration 70 : mse loss = 0.029515674337744713
iteration 80 : mse loss = 0.038930416107177734
iteration 90 : mse loss = 0.030629923567175865
iteration 100 : mse loss = 0.0341804102063179
iteration 110 : mse loss = 0.038414519280195236
iteration 120 : mse loss = 0.02958504855632782
iteration 130 : mse loss = 0.035311199724674225
iteration 140 : mse loss = 0.039293624460697174
iteration 150 : mse loss = 0.03499933332204819
train mean loss: 0.036044392734766006
early_stop_time/early_stop_threshold: 20 / 40, valid mean loss: 0.034654222428798676
Training Epoch 49 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03599124401807785
iteration 20 : mse loss = 0.038808345794677734
iteration 30 : mse loss = 0.03325553238391876
iteration 40 : mse loss = 0.033352479338645935
iteration 50 : mse loss = 0.03576024994254112
iteration 60 : mse loss = 0.030032571405172348
iteration 70 : mse loss = 0.04590682312846184
iteration 80 : mse loss = 0.042528264224529266
iteration 90 : mse loss = 0.03587817773222923
iteration 100 : mse loss = 0.03317344933748245
iteration 110 : mse loss = 0.034344494342803955
iteration 120 : mse loss = 0.03677895665168762
iteration 130 : mse loss = 0.029312169179320335
iteration 140 : mse loss = 0.040901489555835724
iteration 150 : mse loss = 0.035597916692495346
train mean loss: 0.03598586469888687
early_stop_time/early_stop_threshold: 21 / 40, valid mean loss: 0.03674228861927986
Training Epoch 50 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03862333670258522
iteration 20 : mse loss = 0.03466837480664253
iteration 30 : mse loss = 0.03523783013224602
iteration 40 : mse loss = 0.03211367875337601
iteration 50 : mse loss = 0.0423654243350029
iteration 60 : mse loss = 0.03356504812836647
iteration 70 : mse loss = 0.03243336081504822
iteration 80 : mse loss = 0.0378570556640625
iteration 90 : mse loss = 0.03145599365234375
iteration 100 : mse loss = 0.0377710722386837
iteration 110 : mse loss = 0.04549998790025711
iteration 120 : mse loss = 0.03916267678141594
iteration 130 : mse loss = 0.03260146826505661
iteration 140 : mse loss = 0.03245561197400093
iteration 150 : mse loss = 0.03581991791725159
train mean loss: 0.03570195659995079
early_stop_time/early_stop_threshold: 22 / 40, valid mean loss: 0.03468538448214531
Training Epoch 51 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.029435113072395325
iteration 20 : mse loss = 0.032098136842250824
iteration 30 : mse loss = 0.03506334871053696
iteration 40 : mse loss = 0.037307705730199814
iteration 50 : mse loss = 0.029207265004515648
iteration 60 : mse loss = 0.028406668454408646
iteration 70 : mse loss = 0.03275812789797783
iteration 80 : mse loss = 0.029576312750577927
iteration 90 : mse loss = 0.027593135833740234
iteration 100 : mse loss = 0.038115181028842926
iteration 110 : mse loss = 0.040948569774627686
iteration 120 : mse loss = 0.0315183661878109
iteration 130 : mse loss = 0.03291661664843559
iteration 140 : mse loss = 0.025261355563998222
iteration 150 : mse loss = 0.03490610420703888
train mean loss: 0.0344230942428112
early_stop_time/early_stop_threshold: 23 / 40, valid mean loss: 0.0335494726896286
Training Epoch 52 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.028228994458913803
iteration 20 : mse loss = 0.04485582560300827
iteration 30 : mse loss = 0.03238730877637863
iteration 40 : mse loss = 0.026181846857070923
iteration 50 : mse loss = 0.0240748580545187
iteration 60 : mse loss = 0.03813771903514862
iteration 70 : mse loss = 0.03752956539392471
iteration 80 : mse loss = 0.037911683320999146
iteration 90 : mse loss = 0.03460327908396721
iteration 100 : mse loss = 0.0426933690905571
iteration 110 : mse loss = 0.030145233497023582
iteration 120 : mse loss = 0.028981050476431847
iteration 130 : mse loss = 0.04090471193194389
iteration 140 : mse loss = 0.03402840346097946
iteration 150 : mse loss = 0.03876429796218872
train mean loss: 0.03536004200577736
early_stop_time/early_stop_threshold: 24 / 40, valid mean loss: 0.03550796955823898
Training Epoch 53 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.04417089745402336
iteration 20 : mse loss = 0.03393039479851723
iteration 30 : mse loss = 0.03258416801691055
iteration 40 : mse loss = 0.03647173196077347
iteration 50 : mse loss = 0.032368674874305725
iteration 60 : mse loss = 0.033482491970062256
iteration 70 : mse loss = 0.03460097312927246
iteration 80 : mse loss = 0.02966097556054592
iteration 90 : mse loss = 0.038294509053230286
iteration 100 : mse loss = 0.041217025369405746
iteration 110 : mse loss = 0.036757562309503555
iteration 120 : mse loss = 0.035785313695669174
iteration 130 : mse loss = 0.03628847748041153
iteration 140 : mse loss = 0.03724884241819382
iteration 150 : mse loss = 0.036761678755283356
train mean loss: 0.03498774394392967
early_stop_time/early_stop_threshold: 25 / 40, valid mean loss: 0.035353176295757294
Training Epoch 54 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03815120458602905
iteration 20 : mse loss = 0.039980798959732056
iteration 30 : mse loss = 0.03483323007822037
iteration 40 : mse loss = 0.027141129598021507
iteration 50 : mse loss = 0.03496091067790985
iteration 60 : mse loss = 0.02874111197888851
iteration 70 : mse loss = 0.03410949930548668
iteration 80 : mse loss = 0.03175784647464752
iteration 90 : mse loss = 0.04006778076291084
iteration 100 : mse loss = 0.03109469637274742
iteration 110 : mse loss = 0.033950433135032654
iteration 120 : mse loss = 0.022911105304956436
iteration 130 : mse loss = 0.03474809229373932
iteration 140 : mse loss = 0.03545820340514183
iteration 150 : mse loss = 0.03925566375255585
train mean loss: 0.03453337773680687
early_stop_time/early_stop_threshold: 26 / 40, valid mean loss: 0.034361910074949265
Training Epoch 55 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.02590383216738701
iteration 20 : mse loss = 0.03109419345855713
iteration 30 : mse loss = 0.035029344260692596
iteration 40 : mse loss = 0.040407776832580566
iteration 50 : mse loss = 0.03841466084122658
iteration 60 : mse loss = 0.034854378551244736
iteration 70 : mse loss = 0.03968433290719986
iteration 80 : mse loss = 0.03706773743033409
iteration 90 : mse loss = 0.030568674206733704
iteration 100 : mse loss = 0.027803463861346245
iteration 110 : mse loss = 0.03205183520913124
iteration 120 : mse loss = 0.02875494584441185
iteration 130 : mse loss = 0.034317098557949066
iteration 140 : mse loss = 0.029790064319968224
iteration 150 : mse loss = 0.033724576234817505
train mean loss: 0.03471144661307335
early_stop_time/early_stop_threshold: 27 / 40, valid mean loss: 0.034774526953697205
Training Epoch 56 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.039320506155490875
iteration 20 : mse loss = 0.02785230055451393
iteration 30 : mse loss = 0.0312921404838562
iteration 40 : mse loss = 0.03673076629638672
iteration 50 : mse loss = 0.034680649638175964
iteration 60 : mse loss = 0.03352564200758934
iteration 70 : mse loss = 0.031733084470033646
iteration 80 : mse loss = 0.032222725450992584
iteration 90 : mse loss = 0.02535831928253174
iteration 100 : mse loss = 0.024852661415934563
iteration 110 : mse loss = 0.03840276971459389
iteration 120 : mse loss = 0.03629506379365921
iteration 130 : mse loss = 0.03653481602668762
iteration 140 : mse loss = 0.03612169250845909
iteration 150 : mse loss = 0.03857099637389183
train mean loss: 0.033966075628995895
early_stop_time/early_stop_threshold: 28 / 40, valid mean loss: 0.033649981021881104
Training Epoch 57 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.037861473858356476
iteration 20 : mse loss = 0.03218447417020798
iteration 30 : mse loss = 0.04745107144117355
iteration 40 : mse loss = 0.028577860444784164
iteration 50 : mse loss = 0.03270624950528145
iteration 60 : mse loss = 0.03630155324935913
iteration 70 : mse loss = 0.037207212299108505
iteration 80 : mse loss = 0.03355421870946884
iteration 90 : mse loss = 0.030891284346580505
iteration 100 : mse loss = 0.022663399577140808
iteration 110 : mse loss = 0.032799817621707916
iteration 120 : mse loss = 0.0252685584127903
iteration 130 : mse loss = 0.042264174669981
iteration 140 : mse loss = 0.035765450447797775
iteration 150 : mse loss = 0.03151979669928551
train mean loss: 0.03463625907897949
early_stop_time/early_stop_threshold: 29 / 40, valid mean loss: 0.03371967002749443
Training Epoch 58 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03527270257472992
iteration 20 : mse loss = 0.030390730127692223
iteration 30 : mse loss = 0.03566564619541168
iteration 40 : mse loss = 0.03196810185909271
iteration 50 : mse loss = 0.029280973598361015
iteration 60 : mse loss = 0.03332306817173958
iteration 70 : mse loss = 0.03987857699394226
iteration 80 : mse loss = 0.028791075572371483
iteration 90 : mse loss = 0.03513265773653984
iteration 100 : mse loss = 0.04026351496577263
iteration 110 : mse loss = 0.031839024275541306
iteration 120 : mse loss = 0.03137069195508957
iteration 130 : mse loss = 0.03968556225299835
iteration 140 : mse loss = 0.035063471645116806
iteration 150 : mse loss = 0.031059181317687035
train mean loss: 0.034407589584589005
early_stop_time/early_stop_threshold: 30 / 40, valid mean loss: 0.03432951495051384
Training Epoch 59 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.036700598895549774
iteration 20 : mse loss = 0.043789323419332504
iteration 30 : mse loss = 0.03120036981999874
iteration 40 : mse loss = 0.03219906985759735
iteration 50 : mse loss = 0.025376170873641968
iteration 60 : mse loss = 0.02958068996667862
iteration 70 : mse loss = 0.03833387419581413
iteration 80 : mse loss = 0.0287921242415905
iteration 90 : mse loss = 0.034333210438489914
iteration 100 : mse loss = 0.03268972784280777
iteration 110 : mse loss = 0.04059063643217087
iteration 120 : mse loss = 0.030079778283834457
iteration 130 : mse loss = 0.03222566843032837
iteration 140 : mse loss = 0.02386624738574028
iteration 150 : mse loss = 0.03801043704152107
train mean loss: 0.03406211733818054
early_stop_time/early_stop_threshold: 31 / 40, valid mean loss: 0.03440681844949722
Training Epoch 60 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03584212809801102
iteration 20 : mse loss = 0.03756202012300491
iteration 30 : mse loss = 0.03404691815376282
iteration 40 : mse loss = 0.03740835189819336
iteration 50 : mse loss = 0.030795101076364517
iteration 60 : mse loss = 0.03507637605071068
iteration 70 : mse loss = 0.02811272069811821
iteration 80 : mse loss = 0.03747434541583061
iteration 90 : mse loss = 0.037381432950496674
iteration 100 : mse loss = 0.033700063824653625
iteration 110 : mse loss = 0.04064610227942467
iteration 120 : mse loss = 0.0445781871676445
iteration 130 : mse loss = 0.03613768145442009
iteration 140 : mse loss = 0.03183335065841675
iteration 150 : mse loss = 0.027209553867578506
train mean loss: 0.03401292487978935
early_stop_time/early_stop_threshold: 32 / 40, valid mean loss: 0.03396410495042801
Training Epoch 61 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03429427742958069
iteration 20 : mse loss = 0.036195844411849976
iteration 30 : mse loss = 0.030234722420573235
iteration 40 : mse loss = 0.032894913107156754
iteration 50 : mse loss = 0.03477178514003754
iteration 60 : mse loss = 0.041151802986860275
iteration 70 : mse loss = 0.040227118879556656
iteration 80 : mse loss = 0.03428005799651146
iteration 90 : mse loss = 0.03550159931182861
iteration 100 : mse loss = 0.034499309957027435
iteration 110 : mse loss = 0.032451774924993515
iteration 120 : mse loss = 0.03659914806485176
iteration 130 : mse loss = 0.0314231812953949
iteration 140 : mse loss = 0.031499288976192474
iteration 150 : mse loss = 0.045713555067777634
train mean loss: 0.03380730748176575
early_stop_time/early_stop_threshold: 33 / 40, valid mean loss: 0.032957326620817184
Training Epoch 62 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.030162658542394638
iteration 20 : mse loss = 0.03521689027547836
iteration 30 : mse loss = 0.03553977236151695
iteration 40 : mse loss = 0.028926413506269455
iteration 50 : mse loss = 0.032231904566287994
iteration 60 : mse loss = 0.0357549712061882
iteration 70 : mse loss = 0.03804249316453934
iteration 80 : mse loss = 0.03756314143538475
iteration 90 : mse loss = 0.039527662098407745
iteration 100 : mse loss = 0.03533905744552612
iteration 110 : mse loss = 0.03949315473437309
iteration 120 : mse loss = 0.03144294396042824
iteration 130 : mse loss = 0.031933244317770004
iteration 140 : mse loss = 0.04108957573771477
iteration 150 : mse loss = 0.03664609044790268
train mean loss: 0.03386404737830162
early_stop_time/early_stop_threshold: 34 / 40, valid mean loss: 0.03320758789777756
Training Epoch 63 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03558430075645447
iteration 20 : mse loss = 0.029570642858743668
iteration 30 : mse loss = 0.03267880156636238
iteration 40 : mse loss = 0.04601875692605972
iteration 50 : mse loss = 0.03524647653102875
iteration 60 : mse loss = 0.031054701656103134
iteration 70 : mse loss = 0.03502804785966873
iteration 80 : mse loss = 0.030163582414388657
iteration 90 : mse loss = 0.027579274028539658
iteration 100 : mse loss = 0.038835372775793076
iteration 110 : mse loss = 0.03121081367135048
iteration 120 : mse loss = 0.03973225876688957
iteration 130 : mse loss = 0.027922790497541428
iteration 140 : mse loss = 0.0375845804810524
iteration 150 : mse loss = 0.029712572693824768
train mean loss: 0.033501047641038895
early_stop_time/early_stop_threshold: 35 / 40, valid mean loss: 0.03446263074874878
Training Epoch 64 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.023662563413381577
iteration 20 : mse loss = 0.02863398753106594
iteration 30 : mse loss = 0.03251633420586586
iteration 40 : mse loss = 0.036141082644462585
iteration 50 : mse loss = 0.03226407617330551
iteration 60 : mse loss = 0.03183778375387192
iteration 70 : mse loss = 0.04603823274374008
iteration 80 : mse loss = 0.03166164457798004
iteration 90 : mse loss = 0.03553248941898346
iteration 100 : mse loss = 0.035883378237485886
iteration 110 : mse loss = 0.03336908295750618
iteration 120 : mse loss = 0.0372626855969429
iteration 130 : mse loss = 0.03225807845592499
iteration 140 : mse loss = 0.04011937603354454
iteration 150 : mse loss = 0.03580468147993088
train mean loss: 0.0342940129339695
early_stop_time/early_stop_threshold: 36 / 40, valid mean loss: 0.03390255197882652
Training Epoch 65 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.041058849543333054
iteration 20 : mse loss = 0.03365151584148407
iteration 30 : mse loss = 0.0312933512032032
iteration 40 : mse loss = 0.03358355909585953
iteration 50 : mse loss = 0.031383633613586426
iteration 60 : mse loss = 0.035574398934841156
iteration 70 : mse loss = 0.03324022516608238
iteration 80 : mse loss = 0.03543704003095627
iteration 90 : mse loss = 0.03594689071178436
iteration 100 : mse loss = 0.029322749003767967
iteration 110 : mse loss = 0.03036234900355339
iteration 120 : mse loss = 0.03500131890177727
iteration 130 : mse loss = 0.03084002435207367
iteration 140 : mse loss = 0.03333842754364014
iteration 150 : mse loss = 0.028007056564092636
train mean loss: 0.03379343822598457
early_stop_time/early_stop_threshold: 37 / 40, valid mean loss: 0.0335562601685524
Training Epoch 66 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.027874674648046494
iteration 20 : mse loss = 0.037908416241407394
iteration 30 : mse loss = 0.03542080149054527
iteration 40 : mse loss = 0.03207267448306084
iteration 50 : mse loss = 0.025695238262414932
iteration 60 : mse loss = 0.029112204909324646
iteration 70 : mse loss = 0.025859490036964417
iteration 80 : mse loss = 0.03683384507894516
iteration 90 : mse loss = 0.030871640890836716
iteration 100 : mse loss = 0.03448937088251114
iteration 110 : mse loss = 0.03098529390990734
iteration 120 : mse loss = 0.030763020738959312
iteration 130 : mse loss = 0.036440759897232056
iteration 140 : mse loss = 0.030771395191550255
iteration 150 : mse loss = 0.03679560869932175
train mean loss: 0.03351176530122757
early_stop_time/early_stop_threshold: 38 / 40, valid mean loss: 0.03316343575716019
Training Epoch 67 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.02657271735370159
iteration 20 : mse loss = 0.028956681489944458
iteration 30 : mse loss = 0.036407455801963806
iteration 40 : mse loss = 0.033229000866413116
iteration 50 : mse loss = 0.03794082999229431
iteration 60 : mse loss = 0.034321703016757965
iteration 70 : mse loss = 0.02790883742272854
iteration 80 : mse loss = 0.03164098039269447
iteration 90 : mse loss = 0.029619377106428146
iteration 100 : mse loss = 0.027525246143341064
iteration 110 : mse loss = 0.03753829747438431
iteration 120 : mse loss = 0.03275712952017784
iteration 130 : mse loss = 0.0305427648127079
iteration 140 : mse loss = 0.03567955642938614
iteration 150 : mse loss = 0.031235720962285995
train mean loss: 0.03330763801932335
early_stop_time/early_stop_threshold: 39 / 40, valid mean loss: 0.0344855859875679
Training Epoch 68 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.030424071475863457
iteration 20 : mse loss = 0.03527899831533432
iteration 30 : mse loss = 0.02863847278058529
iteration 40 : mse loss = 0.03079015575349331
iteration 50 : mse loss = 0.0445852093398571
iteration 60 : mse loss = 0.040532395243644714
iteration 70 : mse loss = 0.039324164390563965
iteration 80 : mse loss = 0.02891462668776512
iteration 90 : mse loss = 0.04176420718431473
iteration 100 : mse loss = 0.035169344395399094
iteration 110 : mse loss = 0.034971706569194794
iteration 120 : mse loss = 0.0355394072830677
iteration 130 : mse loss = 0.029983749613165855
iteration 140 : mse loss = 0.039674706757068634
iteration 150 : mse loss = 0.04049164801836014
train mean loss: 0.0331987626850605
early_stop_time/early_stop_threshold: 40 / 40, valid mean loss: 0.034772079437971115
Training Epoch 69 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03929886221885681
iteration 20 : mse loss = 0.03308941051363945
iteration 30 : mse loss = 0.03369360789656639
iteration 40 : mse loss = 0.03509414196014404
iteration 50 : mse loss = 0.031202983111143112
iteration 60 : mse loss = 0.031361423432826996
iteration 70 : mse loss = 0.033011823892593384
iteration 80 : mse loss = 0.0319196954369545
iteration 90 : mse loss = 0.021387368440628052
iteration 100 : mse loss = 0.03519562631845474
iteration 110 : mse loss = 0.029894812032580376
iteration 120 : mse loss = 0.028474953025579453
iteration 130 : mse loss = 0.025866230949759483
iteration 140 : mse loss = 0.029901159927248955
iteration 150 : mse loss = 0.03199951350688934
train mean loss: 0.032795462757349014
early_stop_time/early_stop_threshold: 0 / 40, valid mean loss: 0.03283800929784775
Training Epoch 70 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.030987611040472984
iteration 20 : mse loss = 0.0413748100399971
iteration 30 : mse loss = 0.031046157702803612
iteration 40 : mse loss = 0.026288311928510666
iteration 50 : mse loss = 0.03428729623556137
iteration 60 : mse loss = 0.03526943176984787
iteration 70 : mse loss = 0.028824716806411743
iteration 80 : mse loss = 0.0360523946583271
iteration 90 : mse loss = 0.042072322219610214
iteration 100 : mse loss = 0.03508816286921501
iteration 110 : mse loss = 0.03976483270525932
iteration 120 : mse loss = 0.033871378749608994
iteration 130 : mse loss = 0.0371004194021225
iteration 140 : mse loss = 0.03423544391989708
iteration 150 : mse loss = 0.03229551017284393
train mean loss: 0.033232301473617554
early_stop_time/early_stop_threshold: 1 / 40, valid mean loss: 0.03473963588476181
Training Epoch 71 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.035970963537693024
iteration 20 : mse loss = 0.03930364549160004
iteration 30 : mse loss = 0.037690114229917526
iteration 40 : mse loss = 0.038625773042440414
iteration 50 : mse loss = 0.03623180463910103
iteration 60 : mse loss = 0.044956594705581665
iteration 70 : mse loss = 0.049429088830947876
iteration 80 : mse loss = 0.04513074457645416
iteration 90 : mse loss = 0.045780353248119354
iteration 100 : mse loss = 0.05322038382291794
iteration 110 : mse loss = 0.04804093390703201
iteration 120 : mse loss = 0.037728287279605865
iteration 130 : mse loss = 0.03468882292509079
iteration 140 : mse loss = 0.03097286820411682
iteration 150 : mse loss = 0.04356805980205536
train mean loss: 0.04173573851585388
early_stop_time/early_stop_threshold: 2 / 40, valid mean loss: 0.03483789414167404
Training Epoch 72 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03557787835597992
iteration 20 : mse loss = 0.03221607208251953
iteration 30 : mse loss = 0.04289799928665161
iteration 40 : mse loss = 0.027916356921195984
iteration 50 : mse loss = 0.04554365202784538
iteration 60 : mse loss = 0.03276856988668442
iteration 70 : mse loss = 0.03539104759693146
iteration 80 : mse loss = 0.04069351404905319
iteration 90 : mse loss = 0.046605050563812256
iteration 100 : mse loss = 0.03496766835451126
iteration 110 : mse loss = 0.040347643196582794
iteration 120 : mse loss = 0.03697168827056885
iteration 130 : mse loss = 0.02276933752000332
iteration 140 : mse loss = 0.0351463258266449
iteration 150 : mse loss = 0.029454972594976425
train mean loss: 0.03541274741292
early_stop_time/early_stop_threshold: 3 / 40, valid mean loss: 0.03508882597088814
Training Epoch 73 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03898189961910248
iteration 20 : mse loss = 0.03301113843917847
iteration 30 : mse loss = 0.039443548768758774
iteration 40 : mse loss = 0.03138817474246025
iteration 50 : mse loss = 0.03510163351893425
iteration 60 : mse loss = 0.03530970588326454
iteration 70 : mse loss = 0.026649227365851402
iteration 80 : mse loss = 0.03370845317840576
iteration 90 : mse loss = 0.04185767471790314
iteration 100 : mse loss = 0.03709475323557854
iteration 110 : mse loss = 0.03139759600162506
iteration 120 : mse loss = 0.035769831389188766
iteration 130 : mse loss = 0.026855159550905228
iteration 140 : mse loss = 0.033465150743722916
iteration 150 : mse loss = 0.03479492664337158
train mean loss: 0.03432934731245041
early_stop_time/early_stop_threshold: 4 / 40, valid mean loss: 0.033755745738744736
Training Epoch 74 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.04341816157102585
iteration 20 : mse loss = 0.03339250385761261
iteration 30 : mse loss = 0.03649475798010826
iteration 40 : mse loss = 0.02908179722726345
iteration 50 : mse loss = 0.048006944358348846
iteration 60 : mse loss = 0.03645354136824608
iteration 70 : mse loss = 0.039853841066360474
iteration 80 : mse loss = 0.04108108580112457
iteration 90 : mse loss = 0.02923240140080452
iteration 100 : mse loss = 0.03849866986274719
iteration 110 : mse loss = 0.036433979868888855
iteration 120 : mse loss = 0.029453996568918228
iteration 130 : mse loss = 0.03195473551750183
iteration 140 : mse loss = 0.026415159925818443
iteration 150 : mse loss = 0.02706224098801613
train mean loss: 0.03425487130880356
early_stop_time/early_stop_threshold: 5 / 40, valid mean loss: 0.03486189618706703
Training Epoch 75 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03370107710361481
iteration 20 : mse loss = 0.027907565236091614
iteration 30 : mse loss = 0.03473616763949394
iteration 40 : mse loss = 0.030282575637102127
iteration 50 : mse loss = 0.029428858309984207
iteration 60 : mse loss = 0.029857292771339417
iteration 70 : mse loss = 0.036139171570539474
iteration 80 : mse loss = 0.04199264943599701
iteration 90 : mse loss = 0.035945262759923935
iteration 100 : mse loss = 0.03080132231116295
iteration 110 : mse loss = 0.030080778524279594
iteration 120 : mse loss = 0.03603833541274071
iteration 130 : mse loss = 0.03607261925935745
iteration 140 : mse loss = 0.030087217688560486
iteration 150 : mse loss = 0.04185493290424347
train mean loss: 0.03578677400946617
early_stop_time/early_stop_threshold: 6 / 40, valid mean loss: 0.03599648177623749
Training Epoch 76 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.030689075589179993
iteration 20 : mse loss = 0.03207990527153015
iteration 30 : mse loss = 0.032732293009757996
iteration 40 : mse loss = 0.04349561780691147
iteration 50 : mse loss = 0.03645431995391846
iteration 60 : mse loss = 0.030762553215026855
iteration 70 : mse loss = 0.028432263061404228
iteration 80 : mse loss = 0.03676675632596016
iteration 90 : mse loss = 0.03334888070821762
iteration 100 : mse loss = 0.040200941264629364
iteration 110 : mse loss = 0.03224487975239754
iteration 120 : mse loss = 0.03915095329284668
iteration 130 : mse loss = 0.03114595077931881
iteration 140 : mse loss = 0.0346357487142086
iteration 150 : mse loss = 0.0355144739151001
train mean loss: 0.03538360819220543
early_stop_time/early_stop_threshold: 7 / 40, valid mean loss: 0.03468233346939087
Training Epoch 77 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03231232613325119
iteration 20 : mse loss = 0.03259589523077011
iteration 30 : mse loss = 0.03854045644402504
iteration 40 : mse loss = 0.03462713211774826
iteration 50 : mse loss = 0.02521969936788082
iteration 60 : mse loss = 0.033934615552425385
iteration 70 : mse loss = 0.032969407737255096
iteration 80 : mse loss = 0.034075673669576645
iteration 90 : mse loss = 0.043405622243881226
iteration 100 : mse loss = 0.03473607823252678
iteration 110 : mse loss = 0.04517489671707153
iteration 120 : mse loss = 0.027740424498915672
iteration 130 : mse loss = 0.03611753508448601
iteration 140 : mse loss = 0.03303752839565277
iteration 150 : mse loss = 0.024053961038589478
train mean loss: 0.03429656848311424
early_stop_time/early_stop_threshold: 8 / 40, valid mean loss: 0.03628877177834511
Training Epoch 78 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.02618635818362236
iteration 20 : mse loss = 0.029600342735648155
iteration 30 : mse loss = 0.03764696419239044
iteration 40 : mse loss = 0.03399963304400444
iteration 50 : mse loss = 0.03210688754916191
iteration 60 : mse loss = 0.03714200481772423
iteration 70 : mse loss = 0.04050999879837036
iteration 80 : mse loss = 0.02958366647362709
iteration 90 : mse loss = 0.03573624789714813
iteration 100 : mse loss = 0.03599437698721886
iteration 110 : mse loss = 0.04044017940759659
iteration 120 : mse loss = 0.03199513256549835
iteration 130 : mse loss = 0.047005049884319305
iteration 140 : mse loss = 0.02734268270432949
iteration 150 : mse loss = 0.037459731101989746
train mean loss: 0.03354507312178612
early_stop_time/early_stop_threshold: 9 / 40, valid mean loss: 0.03563162311911583
Training Epoch 79 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.0411769300699234
iteration 20 : mse loss = 0.02613678202033043
iteration 30 : mse loss = 0.05216054618358612
iteration 40 : mse loss = 0.05212540179491043
iteration 50 : mse loss = 0.05086348578333855
iteration 60 : mse loss = 0.05082092806696892
iteration 70 : mse loss = 0.04480808228254318
iteration 80 : mse loss = 0.05429188907146454
iteration 90 : mse loss = 0.04342394694685936
iteration 100 : mse loss = 0.032229941338300705
iteration 110 : mse loss = 0.0432933084666729
iteration 120 : mse loss = 0.04878152534365654
iteration 130 : mse loss = 0.038891345262527466
iteration 140 : mse loss = 0.03710669279098511
iteration 150 : mse loss = 0.04751800000667572
train mean loss: 0.050543058663606644
early_stop_time/early_stop_threshold: 10 / 40, valid mean loss: 0.04034272953867912
Training Epoch 80 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.04230087250471115
iteration 20 : mse loss = 0.04323549196124077
iteration 30 : mse loss = 0.03858369588851929
iteration 40 : mse loss = 0.03557369112968445
iteration 50 : mse loss = 0.03188784420490265
iteration 60 : mse loss = 0.03938291221857071
iteration 70 : mse loss = 0.038822486996650696
iteration 80 : mse loss = 0.03246860206127167
iteration 90 : mse loss = 0.037923552095890045
iteration 100 : mse loss = 0.029401831328868866
iteration 110 : mse loss = 0.039790235459804535
iteration 120 : mse loss = 0.03292256221175194
iteration 130 : mse loss = 0.03365467116236687
iteration 140 : mse loss = 0.04005294665694237
iteration 150 : mse loss = 0.03894226253032684
train mean loss: 0.03849799931049347
early_stop_time/early_stop_threshold: 11 / 40, valid mean loss: 0.03774159029126167
Training Epoch 81 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.04521375894546509
iteration 20 : mse loss = 0.03438010811805725
iteration 30 : mse loss = 0.04280507192015648
iteration 40 : mse loss = 0.039251185953617096
iteration 50 : mse loss = 0.029174920171499252
iteration 60 : mse loss = 0.037350840866565704
iteration 70 : mse loss = 0.03554009646177292
iteration 80 : mse loss = 0.04086584970355034
iteration 90 : mse loss = 0.03915778547525406
iteration 100 : mse loss = 0.03109821304678917
iteration 110 : mse loss = 0.03668271005153656
iteration 120 : mse loss = 0.03577064722776413
iteration 130 : mse loss = 0.03486974537372589
iteration 140 : mse loss = 0.04070700705051422
iteration 150 : mse loss = 0.03951702266931534
train mean loss: 0.03768254816532135
early_stop_time/early_stop_threshold: 12 / 40, valid mean loss: 0.036964528262615204
Training Epoch 82 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.0338861458003521
iteration 20 : mse loss = 0.0366266630589962
iteration 30 : mse loss = 0.03397975116968155
iteration 40 : mse loss = 0.032671332359313965
iteration 50 : mse loss = 0.03463209420442581
iteration 60 : mse loss = 0.029449421912431717
iteration 70 : mse loss = 0.03557499498128891
iteration 80 : mse loss = 0.033452898263931274
iteration 90 : mse loss = 0.03946758806705475
iteration 100 : mse loss = 0.035880040377378464
iteration 110 : mse loss = 0.03858903422951698
iteration 120 : mse loss = 0.03480786085128784
iteration 130 : mse loss = 0.03787529468536377
iteration 140 : mse loss = 0.03329642862081528
iteration 150 : mse loss = 0.03442447632551193
train mean loss: 0.03620557859539986
early_stop_time/early_stop_threshold: 13 / 40, valid mean loss: 0.0350155346095562
Training Epoch 83 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03756348788738251
iteration 20 : mse loss = 0.03106187842786312
iteration 30 : mse loss = 0.03558410704135895
iteration 40 : mse loss = 0.04265352338552475
iteration 50 : mse loss = 0.030701329931616783
iteration 60 : mse loss = 0.04485959932208061
iteration 70 : mse loss = 0.04147493839263916
iteration 80 : mse loss = 0.03294866532087326
iteration 90 : mse loss = 0.0302736833691597
iteration 100 : mse loss = 0.03388488292694092
iteration 110 : mse loss = 0.03478741645812988
iteration 120 : mse loss = 0.04477255791425705
iteration 130 : mse loss = 0.033022068440914154
iteration 140 : mse loss = 0.03429306298494339
iteration 150 : mse loss = 0.040069468319416046
train mean loss: 0.03589847683906555
early_stop_time/early_stop_threshold: 14 / 40, valid mean loss: 0.0370517261326313
Training Epoch 84 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.029015984386205673
iteration 20 : mse loss = 0.029066434130072594
iteration 30 : mse loss = 0.04242759943008423
iteration 40 : mse loss = 0.028013547882437706
iteration 50 : mse loss = 0.04505513608455658
iteration 60 : mse loss = 0.029085543006658554
iteration 70 : mse loss = 0.027570173144340515
iteration 80 : mse loss = 0.033282410353422165
iteration 90 : mse loss = 0.03204791247844696
iteration 100 : mse loss = 0.038917526602745056
iteration 110 : mse loss = 0.04134054109454155
iteration 120 : mse loss = 0.03824944794178009
iteration 130 : mse loss = 0.047744326293468475
iteration 140 : mse loss = 0.041181016713380814
iteration 150 : mse loss = 0.044116467237472534
train mean loss: 0.036984335631132126
early_stop_time/early_stop_threshold: 15 / 40, valid mean loss: 0.037593577057123184
Training Epoch 85 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.036214958876371384
iteration 20 : mse loss = 0.03671800717711449
iteration 30 : mse loss = 0.037744514644145966
iteration 40 : mse loss = 0.04286281391978264
iteration 50 : mse loss = 0.03772585093975067
iteration 60 : mse loss = 0.032254815101623535
iteration 70 : mse loss = 0.03199934586882591
iteration 80 : mse loss = 0.038990892469882965
iteration 90 : mse loss = 0.028315311297774315
iteration 100 : mse loss = 0.04145371541380882
iteration 110 : mse loss = 0.03759101778268814
iteration 120 : mse loss = 0.03872276097536087
iteration 130 : mse loss = 0.03919912874698639
iteration 140 : mse loss = 0.02468182146549225
iteration 150 : mse loss = 0.03429064154624939
train mean loss: 0.03544463962316513
early_stop_time/early_stop_threshold: 16 / 40, valid mean loss: 0.03493718057870865
Training Epoch 86 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.037792690098285675
iteration 20 : mse loss = 0.035630255937576294
iteration 30 : mse loss = 0.03443196043372154
iteration 40 : mse loss = 0.03158783167600632
iteration 50 : mse loss = 0.03344254195690155
iteration 60 : mse loss = 0.029566075652837753
iteration 70 : mse loss = 0.02903696708381176
iteration 80 : mse loss = 0.03823717683553696
iteration 90 : mse loss = 0.0378703847527504
iteration 100 : mse loss = 0.03992529958486557
iteration 110 : mse loss = 0.03360157459974289
iteration 120 : mse loss = 0.03900187835097313
iteration 130 : mse loss = 0.03897649049758911
iteration 140 : mse loss = 0.036604080349206924
iteration 150 : mse loss = 0.02985217794775963
train mean loss: 0.035477414727211
early_stop_time/early_stop_threshold: 17 / 40, valid mean loss: 0.03480413183569908
Training Epoch 87 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03147763013839722
iteration 20 : mse loss = 0.03474745899438858
iteration 30 : mse loss = 0.032041095197200775
iteration 40 : mse loss = 0.029550781473517418
iteration 50 : mse loss = 0.04550667852163315
iteration 60 : mse loss = 0.03645824268460274
iteration 70 : mse loss = 0.035463497042655945
iteration 80 : mse loss = 0.038776397705078125
iteration 90 : mse loss = 0.03381501883268356
iteration 100 : mse loss = 0.039687659591436386
iteration 110 : mse loss = 0.04191126674413681
iteration 120 : mse loss = 0.032832369208335876
iteration 130 : mse loss = 0.04089944437146187
iteration 140 : mse loss = 0.02867179922759533
iteration 150 : mse loss = 0.03769392520189285
train mean loss: 0.03498534858226776
early_stop_time/early_stop_threshold: 18 / 40, valid mean loss: 0.034457992762327194
Training Epoch 88 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03494647145271301
iteration 20 : mse loss = 0.036327313631772995
iteration 30 : mse loss = 0.031584471464157104
iteration 40 : mse loss = 0.03482498601078987
iteration 50 : mse loss = 0.03718036413192749
iteration 60 : mse loss = 0.030419573187828064
iteration 70 : mse loss = 0.035725902765989304
iteration 80 : mse loss = 0.028979327529668808
iteration 90 : mse loss = 0.025448758155107498
iteration 100 : mse loss = 0.03214653208851814
iteration 110 : mse loss = 0.03533771634101868
iteration 120 : mse loss = 0.03705339878797531
iteration 130 : mse loss = 0.03312283381819725
iteration 140 : mse loss = 0.033747509121894836
iteration 150 : mse loss = 0.031023059040308
train mean loss: 0.034325018525123596
early_stop_time/early_stop_threshold: 19 / 40, valid mean loss: 0.03414701297879219
Training Epoch 89 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03793829306960106
iteration 20 : mse loss = 0.03572433441877365
iteration 30 : mse loss = 0.033765874803066254
iteration 40 : mse loss = 0.030712975189089775
iteration 50 : mse loss = 0.03375832363963127
iteration 60 : mse loss = 0.03332032635807991
iteration 70 : mse loss = 0.03440675139427185
iteration 80 : mse loss = 0.0327666811645031
iteration 90 : mse loss = 0.03421104699373245
iteration 100 : mse loss = 0.038403090089559555
iteration 110 : mse loss = 0.03335027024149895
iteration 120 : mse loss = 0.036764174699783325
iteration 130 : mse loss = 0.03413122147321701
iteration 140 : mse loss = 0.039344824850559235
iteration 150 : mse loss = 0.03567957133054733
train mean loss: 0.033905189484357834
early_stop_time/early_stop_threshold: 20 / 40, valid mean loss: 0.0335560217499733
Training Epoch 90 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.04133710265159607
iteration 20 : mse loss = 0.03547826036810875
iteration 30 : mse loss = 0.030996140092611313
iteration 40 : mse loss = 0.03782821446657181
iteration 50 : mse loss = 0.03940783068537712
iteration 60 : mse loss = 0.035343896597623825
iteration 70 : mse loss = 0.03826231509447098
iteration 80 : mse loss = 0.03531908616423607
iteration 90 : mse loss = 0.03554029390215874
iteration 100 : mse loss = 0.039392195641994476
iteration 110 : mse loss = 0.03154449909925461
iteration 120 : mse loss = 0.03744134679436684
iteration 130 : mse loss = 0.030619289726018906
iteration 140 : mse loss = 0.026636315509676933
iteration 150 : mse loss = 0.03721344470977783
train mean loss: 0.034764230251312256
early_stop_time/early_stop_threshold: 21 / 40, valid mean loss: 0.05251637101173401
Training Epoch 91 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.048820119351148605
iteration 20 : mse loss = 0.04544832557439804
iteration 30 : mse loss = 0.04744923859834671
iteration 40 : mse loss = 0.04068217799067497
iteration 50 : mse loss = 0.03544404357671738
iteration 60 : mse loss = 0.03320103883743286
iteration 70 : mse loss = 0.03781452775001526
iteration 80 : mse loss = 0.040915440768003464
iteration 90 : mse loss = 0.04110824316740036
iteration 100 : mse loss = 0.03953130543231964
iteration 110 : mse loss = 0.03635869547724724
iteration 120 : mse loss = 0.04232050105929375
iteration 130 : mse loss = 0.027820926159620285
iteration 140 : mse loss = 0.04014221578836441
iteration 150 : mse loss = 0.04271005839109421
train mean loss: 0.040516503155231476
early_stop_time/early_stop_threshold: 22 / 40, valid mean loss: 0.03538675233721733
Training Epoch 92 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.042397405952215195
iteration 20 : mse loss = 0.04394014924764633
iteration 30 : mse loss = 0.032583363354206085
iteration 40 : mse loss = 0.029372617602348328
iteration 50 : mse loss = 0.025777842849493027
iteration 60 : mse loss = 0.03849221020936966
iteration 70 : mse loss = 0.030618157237768173
iteration 80 : mse loss = 0.03811684995889664
iteration 90 : mse loss = 0.0395481251180172
iteration 100 : mse loss = 0.03215068578720093
iteration 110 : mse loss = 0.03760584443807602
iteration 120 : mse loss = 0.034227292984724045
iteration 130 : mse loss = 0.03268980234861374
iteration 140 : mse loss = 0.03121027909219265
iteration 150 : mse loss = 0.03254876658320427
train mean loss: 0.03613339364528656
early_stop_time/early_stop_threshold: 23 / 40, valid mean loss: 0.034932177513837814
Training Epoch 93 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03980976343154907
iteration 20 : mse loss = 0.03859582170844078
iteration 30 : mse loss = 0.03744583576917648
iteration 40 : mse loss = 0.03313124179840088
iteration 50 : mse loss = 0.035596154630184174
iteration 60 : mse loss = 0.03598620370030403
iteration 70 : mse loss = 0.041777923703193665
iteration 80 : mse loss = 0.03492923080921173
iteration 90 : mse loss = 0.033213891088962555
iteration 100 : mse loss = 0.027779795229434967
iteration 110 : mse loss = 0.029773501679301262
iteration 120 : mse loss = 0.029628029093146324
iteration 130 : mse loss = 0.03871524706482887
iteration 140 : mse loss = 0.04051598161458969
iteration 150 : mse loss = 0.03592647612094879
train mean loss: 0.03518364205956459
early_stop_time/early_stop_threshold: 24 / 40, valid mean loss: 0.03495815396308899
Training Epoch 94 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03585756570100784
iteration 20 : mse loss = 0.038377124816179276
iteration 30 : mse loss = 0.033007048070430756
iteration 40 : mse loss = 0.04023989662528038
iteration 50 : mse loss = 0.034188780933618546
iteration 60 : mse loss = 0.03579948842525482
iteration 70 : mse loss = 0.03388223797082901
iteration 80 : mse loss = 0.02857871912419796
iteration 90 : mse loss = 0.033986493945121765
iteration 100 : mse loss = 0.030872879549860954
iteration 110 : mse loss = 0.051351696252822876
iteration 120 : mse loss = 0.03608424961566925
iteration 130 : mse loss = 0.04125891625881195
iteration 140 : mse loss = 0.02826252207159996
iteration 150 : mse loss = 0.029661616310477257
train mean loss: 0.03475859388709068
early_stop_time/early_stop_threshold: 25 / 40, valid mean loss: 0.03754902258515358
Training Epoch 95 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.04218167066574097
iteration 20 : mse loss = 0.030758081004023552
iteration 30 : mse loss = 0.03121905028820038
iteration 40 : mse loss = 0.029281487688422203
iteration 50 : mse loss = 0.04153028503060341
iteration 60 : mse loss = 0.035543326288461685
iteration 70 : mse loss = 0.030084824189543724
iteration 80 : mse loss = 0.034505490213632584
iteration 90 : mse loss = 0.03704788163304329
iteration 100 : mse loss = 0.028913205489516258
iteration 110 : mse loss = 0.03329896554350853
iteration 120 : mse loss = 0.03050377033650875
iteration 130 : mse loss = 0.04119333624839783
iteration 140 : mse loss = 0.03587368130683899
iteration 150 : mse loss = 0.03349064290523529
train mean loss: 0.03466843068599701
early_stop_time/early_stop_threshold: 26 / 40, valid mean loss: 0.0352306142449379
Training Epoch 96 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03549369424581528
iteration 20 : mse loss = 0.03975950926542282
iteration 30 : mse loss = 0.028720304369926453
iteration 40 : mse loss = 0.0384523868560791
iteration 50 : mse loss = 0.028719231486320496
iteration 60 : mse loss = 0.03194950148463249
iteration 70 : mse loss = 0.04041674733161926
iteration 80 : mse loss = 0.03703846037387848
iteration 90 : mse loss = 0.04432198405265808
iteration 100 : mse loss = 0.038172584027051926
iteration 110 : mse loss = 0.04070180654525757
iteration 120 : mse loss = 0.035174086689949036
iteration 130 : mse loss = 0.03077114187180996
iteration 140 : mse loss = 0.03211788088083267
iteration 150 : mse loss = 0.02940787747502327
train mean loss: 0.035361841320991516
early_stop_time/early_stop_threshold: 27 / 40, valid mean loss: 0.0348825603723526
Training Epoch 97 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.023983245715498924
iteration 20 : mse loss = 0.03747367113828659
iteration 30 : mse loss = 0.0324954055249691
iteration 40 : mse loss = 0.03128684312105179
iteration 50 : mse loss = 0.02961806207895279
iteration 60 : mse loss = 0.18990115821361542
iteration 70 : mse loss = 0.0608808808028698
iteration 80 : mse loss = 0.045684318989515305
iteration 90 : mse loss = 0.048534199595451355
iteration 100 : mse loss = 0.03985948860645294
iteration 110 : mse loss = 0.046450864523649216
iteration 120 : mse loss = 0.04199778288602829
iteration 130 : mse loss = 0.046425215899944305
iteration 140 : mse loss = 0.038953833281993866
iteration 150 : mse loss = 0.031186064705252647
train mean loss: 0.04785970225930214
early_stop_time/early_stop_threshold: 28 / 40, valid mean loss: 0.03724953159689903
Training Epoch 98 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.03340988606214523
iteration 20 : mse loss = 0.0457109734416008
iteration 30 : mse loss = 0.03380053862929344
iteration 40 : mse loss = 0.03846975788474083
iteration 50 : mse loss = 0.033140067011117935
iteration 60 : mse loss = 0.0430273674428463
iteration 70 : mse loss = 0.03923910856246948
iteration 80 : mse loss = 0.03683992102742195
iteration 90 : mse loss = 0.03076501190662384
iteration 100 : mse loss = 0.030325984582304955
iteration 110 : mse loss = 0.026862207800149918
iteration 120 : mse loss = 0.0326884426176548
iteration 130 : mse loss = 0.03536670655012131
iteration 140 : mse loss = 0.03395205736160278
iteration 150 : mse loss = 0.034783750772476196
train mean loss: 0.03602616861462593
early_stop_time/early_stop_threshold: 29 / 40, valid mean loss: 0.035228319466114044
Training Epoch 99 / 200, Current lr = [0.0001]
iteration 10 : mse loss = 0.034817736595869064
iteration 20 : mse loss = 0.03714452683925629
iteration 30 : mse loss = 0.04130095988512039
iteration 40 : mse loss = 0.030469780787825584
iteration 50 : mse loss = 0.03255685418844223
iteration 60 : mse loss = 0.035409554839134216
iteration 70 : mse loss = 0.029795696958899498
iteration 80 : mse loss = 0.034755513072013855
iteration 90 : mse loss = 0.03675387427210808
iteration 100 : mse loss = 0.04809964820742607
iteration 110 : mse loss = 0.03582887724041939
iteration 120 : mse loss = 0.033371057361364365
iteration 130 : mse loss = 0.04015164077281952
iteration 140 : mse loss = 0.03817593306303024
iteration 150 : mse loss = 0.03502138704061508
train mean loss: 0.0356319285929203
early_stop_time/early_stop_threshold: 30 / 40, valid mean loss: 0.035502124577760696
Training Epoch 100 / 200, Current lr = [9.900990099009902e-05]
iteration 10 : mse loss = 0.0316266193985939
iteration 20 : mse loss = 0.03335880488157272
iteration 30 : mse loss = 0.026050452142953873
iteration 40 : mse loss = 0.030846325680613518
iteration 50 : mse loss = 0.036195240914821625
iteration 60 : mse loss = 0.029670795425772667
iteration 70 : mse loss = 0.03370586037635803
iteration 80 : mse loss = 0.029742728918790817
iteration 90 : mse loss = 0.039928484708070755
iteration 100 : mse loss = 0.029777569696307182
iteration 110 : mse loss = 0.03281184285879135
iteration 120 : mse loss = 0.0306955948472023
iteration 130 : mse loss = 0.0345195010304451
iteration 140 : mse loss = 0.03481174260377884
iteration 150 : mse loss = 0.04275687038898468
train mean loss: 0.034527409821748734
early_stop_time/early_stop_threshold: 31 / 40, valid mean loss: 0.034578837454319
Training Epoch 101 / 200, Current lr = [9.801980198019803e-05]
iteration 10 : mse loss = 0.03917701169848442
iteration 20 : mse loss = 0.03618142008781433
iteration 30 : mse loss = 0.02683720551431179
iteration 40 : mse loss = 0.027809780091047287
iteration 50 : mse loss = 0.03631393611431122
iteration 60 : mse loss = 0.03597691282629967
iteration 70 : mse loss = 0.03791211172938347
iteration 80 : mse loss = 0.027083516120910645
iteration 90 : mse loss = 0.03177618980407715
iteration 100 : mse loss = 0.028177589178085327
iteration 110 : mse loss = 0.03148454427719116
iteration 120 : mse loss = 0.03257128596305847
iteration 130 : mse loss = 0.03138124197721481
iteration 140 : mse loss = 0.03756813704967499
iteration 150 : mse loss = 0.028770921751856804
train mean loss: 0.03385248780250549
early_stop_time/early_stop_threshold: 32 / 40, valid mean loss: 0.03384838253259659
Training Epoch 102 / 200, Current lr = [9.702970297029703e-05]
iteration 10 : mse loss = 0.03145701810717583
iteration 20 : mse loss = 0.03253947198390961
iteration 30 : mse loss = 0.03615458309650421
iteration 40 : mse loss = 0.03787592053413391
iteration 50 : mse loss = 0.03221779689192772
iteration 60 : mse loss = 0.030253170058131218
iteration 70 : mse loss = 0.03386208415031433
iteration 80 : mse loss = 0.0311692263931036
iteration 90 : mse loss = 0.036498140543699265
iteration 100 : mse loss = 0.04237832874059677
iteration 110 : mse loss = 0.033044129610061646
iteration 120 : mse loss = 0.027749603614211082
iteration 130 : mse loss = 0.033073823899030685
iteration 140 : mse loss = 0.036290839314460754
iteration 150 : mse loss = 0.036415450274944305
train mean loss: 0.03417370840907097
early_stop_time/early_stop_threshold: 33 / 40, valid mean loss: 0.034527044743299484
Training Epoch 103 / 200, Current lr = [9.603960396039604e-05]
iteration 10 : mse loss = 0.03778113052248955
iteration 20 : mse loss = 0.031220942735671997
iteration 30 : mse loss = 0.036537230014801025
iteration 40 : mse loss = 0.032924409955739975
iteration 50 : mse loss = 0.036706119775772095
iteration 60 : mse loss = 0.03408545255661011
iteration 70 : mse loss = 0.03901822865009308
iteration 80 : mse loss = 0.030493229627609253
iteration 90 : mse loss = 0.028995316475629807
iteration 100 : mse loss = 0.046021074056625366
iteration 110 : mse loss = 0.035221878439188004
iteration 120 : mse loss = 0.03239366039633751
iteration 130 : mse loss = 0.030219167470932007
iteration 140 : mse loss = 0.03193249553442001
iteration 150 : mse loss = 0.02624375931918621
train mean loss: 0.033972982317209244
early_stop_time/early_stop_threshold: 34 / 40, valid mean loss: 0.0342382974922657
Training Epoch 104 / 200, Current lr = [9.504950495049505e-05]
iteration 10 : mse loss = 0.04258643835783005
iteration 20 : mse loss = 0.0313577726483345
iteration 30 : mse loss = 0.0319284088909626
iteration 40 : mse loss = 0.04124944657087326
iteration 50 : mse loss = 0.032784104347229004
iteration 60 : mse loss = 0.03635583445429802
iteration 70 : mse loss = 0.03562484681606293
iteration 80 : mse loss = 0.03187224268913269
iteration 90 : mse loss = 0.030833929777145386
iteration 100 : mse loss = 0.024678759276866913
iteration 110 : mse loss = 0.0395863801240921
iteration 120 : mse loss = 0.03167426958680153
iteration 130 : mse loss = 0.03995783254504204
iteration 140 : mse loss = 0.03417564183473587
iteration 150 : mse loss = 0.02693701907992363
train mean loss: 0.03421532362699509
early_stop_time/early_stop_threshold: 35 / 40, valid mean loss: 0.03413115814328194
Training Epoch 105 / 200, Current lr = [9.405940594059406e-05]
iteration 10 : mse loss = 0.03265208750963211
iteration 20 : mse loss = 0.027567297220230103
iteration 30 : mse loss = 0.03965694084763527
iteration 40 : mse loss = 0.03219149261713028
iteration 50 : mse loss = 0.037423811852931976
iteration 60 : mse loss = 0.04185083135962486
iteration 70 : mse loss = 0.038555897772312164
iteration 80 : mse loss = 0.030875077471137047
iteration 90 : mse loss = 0.04033530503511429
iteration 100 : mse loss = 0.03999932110309601
iteration 110 : mse loss = 0.0369153693318367
iteration 120 : mse loss = 0.03256039693951607
iteration 130 : mse loss = 0.04237512871623039
iteration 140 : mse loss = 0.037730030715465546
iteration 150 : mse loss = 0.03269839286804199
train mean loss: 0.03501376509666443
early_stop_time/early_stop_threshold: 36 / 40, valid mean loss: 0.035136960446834564
Training Epoch 106 / 200, Current lr = [9.306930693069307e-05]
iteration 10 : mse loss = 0.032133549451828
iteration 20 : mse loss = 0.03397774696350098
iteration 30 : mse loss = 0.031787894666194916
iteration 40 : mse loss = 0.03703538328409195
iteration 50 : mse loss = 0.03140953555703163
iteration 60 : mse loss = 0.0366535447537899
iteration 70 : mse loss = 0.03186355531215668
iteration 80 : mse loss = 0.03711986169219017
iteration 90 : mse loss = 0.042186349630355835
iteration 100 : mse loss = 0.03647834062576294
iteration 110 : mse loss = 0.03354410454630852
iteration 120 : mse loss = 0.03089013509452343
iteration 130 : mse loss = 0.03432228043675423
iteration 140 : mse loss = 0.0294293574988842
iteration 150 : mse loss = 0.03400648385286331
train mean loss: 0.03406590223312378
early_stop_time/early_stop_threshold: 37 / 40, valid mean loss: 0.035004206001758575
Training Epoch 107 / 200, Current lr = [9.207920792079209e-05]
iteration 10 : mse loss = 0.03367085009813309
iteration 20 : mse loss = 0.0299589391797781
iteration 30 : mse loss = 0.030410315841436386
iteration 40 : mse loss = 0.03611033782362938
iteration 50 : mse loss = 0.03227668255567551
iteration 60 : mse loss = 0.0324096605181694
iteration 70 : mse loss = 0.037037648260593414
iteration 80 : mse loss = 0.03606859967112541
iteration 90 : mse loss = 0.03666181117296219
iteration 100 : mse loss = 0.030363788828253746
iteration 110 : mse loss = 0.03159552067518234
iteration 120 : mse loss = 0.028978755697607994
iteration 130 : mse loss = 0.03619534894824028
iteration 140 : mse loss = 0.030492395162582397
iteration 150 : mse loss = 0.030470168218016624
train mean loss: 0.034242574125528336
early_stop_time/early_stop_threshold: 38 / 40, valid mean loss: 0.03418787568807602
Training Epoch 108 / 200, Current lr = [9.10891089108911e-05]
iteration 10 : mse loss = 0.03228500112891197
iteration 20 : mse loss = 0.03784272074699402
iteration 30 : mse loss = 0.03974359855055809
iteration 40 : mse loss = 0.04036301374435425
iteration 50 : mse loss = 0.03551679477095604
iteration 60 : mse loss = 0.04475855082273483
iteration 70 : mse loss = 0.0460001640021801
iteration 80 : mse loss = 0.037567004561424255
iteration 90 : mse loss = 0.03840528428554535
iteration 100 : mse loss = 0.039962396025657654
iteration 110 : mse loss = 0.028943561017513275
iteration 120 : mse loss = 0.03956166282296181
iteration 130 : mse loss = 0.04037787765264511
iteration 140 : mse loss = 0.03931644558906555
iteration 150 : mse loss = 0.03520352765917778
train mean loss: 0.036423441022634506
early_stop_time/early_stop_threshold: 39 / 40, valid mean loss: 0.03426610678434372
Training Epoch 109 / 200, Current lr = [9.009900990099011e-05]
iteration 10 : mse loss = 0.03978817164897919
iteration 20 : mse loss = 0.03352861851453781
iteration 30 : mse loss = 0.03607480973005295
iteration 40 : mse loss = 0.03964928165078163
iteration 50 : mse loss = 0.0422920398414135
iteration 60 : mse loss = 0.029847560450434685
iteration 70 : mse loss = 0.03367223963141441
iteration 80 : mse loss = 0.035149917006492615
iteration 90 : mse loss = 0.03346412256360054
iteration 100 : mse loss = 0.029710140079259872
iteration 110 : mse loss = 0.03384075313806534
iteration 120 : mse loss = 0.032430168241262436
iteration 130 : mse loss = 0.027408763766288757
iteration 140 : mse loss = 0.0396529957652092
iteration 150 : mse loss = 0.029709994792938232
train mean loss: 0.034349534660577774
early_stop_time/early_stop_threshold: 40 / 40, valid mean loss: 0.0344095416367054
Training Epoch 110 / 200, Current lr = [8.910891089108912e-05]
iteration 10 : mse loss = 0.029659902676939964
iteration 20 : mse loss = 0.0383567288517952
iteration 30 : mse loss = 0.03153661638498306
iteration 40 : mse loss = 0.027834665030241013
iteration 50 : mse loss = 0.038295768201351166
iteration 60 : mse loss = 0.037729840725660324
iteration 70 : mse loss = 0.030826054513454437
iteration 80 : mse loss = 0.036348603665828705
iteration 90 : mse loss = 0.03335897997021675
iteration 100 : mse loss = 0.027402929961681366
iteration 110 : mse loss = 0.03556744381785393
iteration 120 : mse loss = 0.039738137274980545
iteration 130 : mse loss = 0.035188402980566025
iteration 140 : mse loss = 0.04075969010591507
iteration 150 : mse loss = 0.03669571131467819
train mean loss: 0.033846139907836914